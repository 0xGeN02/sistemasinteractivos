# StudyAI - AplicaciÃ³n de Estudio con IA Local

AplicaciÃ³n web para estudiar con asistencia de IA completamente local usando **Ollama**.

## ðŸš€ Inicio RÃ¡pido

### 1. Instalar Ollama

```bash
# Linux
curl -fsSL https://ollama.com/install.sh | sh

# Mac
brew install ollama

# Windows: Descargar desde https://ollama.com/download
```

### 2. Instalar un modelo de IA

```bash
# Recomendado (mejor calidad)
ollama pull llama3.1

# O si tienes pocos recursos
ollama pull llama3.2:3b
```

### 3. Iniciar Ollama

```bash
ollama serve
```

### 4. Configurar el proyecto

```bash
# Instalar dependencias
npm install

# Iniciar base de datos
docker-compose up -d

# Aplicar migraciones
docker-compose exec -T postgres psql -U postgres -d studyai < prisma/init.sql
docker-compose exec -T postgres psql -U postgres -d studyai < prisma/add_type_column.sql
```

### 5. Verificar configuraciÃ³n de Ollama

```bash
# Script automÃ¡tico de verificaciÃ³n
bash scripts/check-ollama.sh

# O manual
ollama list  # Ver modelos instalados
```

### 6. Ejecutar la aplicaciÃ³n

```bash
npm run dev:full
```

La aplicaciÃ³n estarÃ¡ disponible en:
- Frontend: http://localhost:3000
- Backend: http://localhost:3001

## âš™ï¸ ConfiguraciÃ³n

Edita el archivo `.env` para cambiar el modelo de IA:

```env
# Modelo de Ollama (recomendado: llama3.1, llama3.2:3b, qwen2.5:7b)
OLLAMA_MODEL="llama3.1"
```

## ðŸ“š Modelos Recomendados

| Modelo | TamaÃ±o | RAM | Velocidad | Uso |
|--------|--------|-----|-----------|-----|
| **llama3.1** | ~4.7GB | 8GB+ | Media | â­ Mejor calidad |
| **llama3.2:3b** | ~2GB | 4GB+ | RÃ¡pida | âš¡ Equipos limitados |
| **qwen2.5:7b** | ~4.4GB | 8GB+ | Media | ðŸ“Š Excelente con JSON |
| **phi3:mini** | ~2.3GB | 4GB+ | Muy rÃ¡pida | ðŸƒ Muy ligero |

Ver mÃ¡s detalles en [OLLAMA_SETUP.md](./OLLAMA_SETUP.md)

## ðŸŽ¯ CaracterÃ­sticas

- âœ… **100% Local** - Sin APIs externas, todo en tu mÃ¡quina
- ðŸŽ™ï¸ **GrabaciÃ³n de audio** - Explica el temario con tu voz
- ðŸ¤– **EvaluaciÃ³n con IA** - Ollama analiza tu explicaciÃ³n
- ðŸ“Š **Feedback detallado** - PrecisiÃ³n, conceptos faltantes y errores
- ðŸ’¾ **Base de datos** - Historial de chats y materiales persistentes
- ðŸ“„ **Soporte PDF** - Carga y estudia desde PDFs

## ðŸ› ï¸ TecnologÃ­as

- **Frontend:** React + TypeScript + Vite + TailwindCSS
- **Backend:** Express + TypeScript + PostgreSQL
- **IA:** Ollama (LLama3.1/3.2)
- **Base de datos:** PostgreSQL (Docker)

## ðŸ“– CÃ³mo Usar

1. **Crear un chat** de estudio o prÃ¡ctica
2. **Agregar material** (texto o PDF)
3. **Grabar tu explicaciÃ³n** del temario
4. **Recibir feedback** instantÃ¡neo de la IA

## ðŸ”§ Scripts Ãštiles

```bash
# Desarrollo completo (frontend + backend + hot reload)
npm run dev:full

# Solo frontend
npm run dev

# Solo backend
npm run server

# Verificar Ollama
bash scripts/check-ollama.sh

# Base de datos
docker-compose up -d       # Iniciar
docker-compose down        # Detener
docker-compose logs -f     # Ver logs
```

## ðŸ“ SoluciÃ³n de Problemas

Ver [OLLAMA_SETUP.md](./OLLAMA_SETUP.md) para:
- Problemas con Ollama
- Cambiar modelo de IA
- ConfiguraciÃ³n de Whisper local
- OptimizaciÃ³n de rendimiento

## ðŸ” Privacidad

Todo funciona localmente:
- âœ… Sin enviar datos a servicios externos
- âœ… Sin necesidad de API keys
- âœ… Tus datos nunca salen de tu mÃ¡quina
  