# Configuraci√≥n de Ollama para StudyAI

Este proyecto usa **Ollama** localmente para todas las funciones de IA, sin necesidad de OpenAI.

## üìã Modelos Recomendados

### Para Evaluaci√≥n de Recitaciones (usado actualmente)

El proyecto est√° configurado para usar `llama3.1` por defecto. Aqu√≠ las opciones:

#### ü•á Recomendado: **llama3.1:8b** o **llama3.2:3b**
```bash
# Instalar llama3.1 (modelo de 8B par√°metros - ~4.7GB)
ollama pull llama3.1

# O llama3.2 (m√°s ligero, 3B par√°metros - ~2GB)
ollama pull llama3.2:3b
```

**Ventajas:**
- Excelente comprensi√≥n de texto en espa√±ol
- Bueno siguiendo instrucciones de formato JSON
- Balance perfecto entre calidad y velocidad

#### ü•à Alternativa: **qwen2.5:7b**
```bash
ollama pull qwen2.5:7b
```

**Ventajas:**
- Muy bueno con instrucciones estructuradas
- Excelente en generar JSON
- R√°pido

#### ü•â Opci√≥n Ligera: **phi3:mini**
```bash
ollama pull phi3:mini
```

**Ventajas:**
- Muy r√°pido (~2GB)
- Bueno para feedback b√°sico
- Ideal para equipos con pocos recursos

## üöÄ Comandos √ötiles

### Ver modelos instalados
```bash
ollama list
```

### Instalar un modelo
```bash
ollama pull <nombre-del-modelo>
```

### Probar un modelo
```bash
ollama run llama3.1
```

### Eliminar un modelo que no uses
```bash
ollama rm <nombre-del-modelo>
```

## ‚öôÔ∏è Cambiar el Modelo en el Proyecto

Edita el archivo `reciteRoutes.ts` y cambia la l√≠nea:

```typescript
const response = await ollama.chat({
  model: "llama3.1",  // <-- Cambia aqu√≠ el nombre del modelo
  messages: [{ role: "user", content: prompt }],
});
```

Puedes usar cualquier modelo que tengas instalado:
- `llama3.1`
- `llama3.2:3b`
- `qwen2.5:7b`
- `phi3:mini`
- etc.

## üîß Variables de Entorno

En tu archivo `.env`, puedes configurar:

```env
# NO necesitas estas si usas solo Ollama:
# OPENAI_API_KEY=sk-...
# LOCAL_WHISPER_URL=http://...

# Ollama se conecta autom√°ticamente a localhost:11434
```

## üìä Comparaci√≥n de Modelos

| Modelo | Tama√±o | RAM Recomendada | Velocidad | Calidad |
|--------|--------|-----------------|-----------|---------|
| llama3.1 | ~4.7GB | 8GB+ | Media | Excelente |
| llama3.2:3b | ~2GB | 4GB+ | R√°pida | Muy Buena |
| qwen2.5:7b | ~4.4GB | 8GB+ | Media | Excelente |
| phi3:mini | ~2.3GB | 4GB+ | Muy R√°pida | Buena |

## üéØ Recomendaci√≥n Final

**Para mejor experiencia:**
```bash
ollama pull llama3.1
```

**Si tienes PC con pocos recursos:**
```bash
ollama pull llama3.2:3b
```

## üêõ Soluci√≥n de Problemas

### Ollama no responde
```bash
# Verificar que Ollama est√° corriendo
ollama serve

# O en segundo plano (Linux/Mac)
ollama serve &
```

### El modelo no genera JSON v√°lido
- Prueba con `qwen2.5:7b` que es mejor siguiendo formatos estructurados
- O mejora el prompt en `reciteRoutes.ts`

### Respuestas muy lentas
- Usa un modelo m√°s peque√±o como `llama3.2:3b` o `phi3:mini`
- Verifica que no est√©s usando CPU en lugar de GPU

## üéôÔ∏è Transcripci√≥n de Audio

El proyecto usa **Web Speech API** integrada en el navegador:

- ‚úÖ **Gratis** - Sin costo
- ‚úÖ **Sin configuraci√≥n** - Ya incluida en Chrome/Edge
- ‚úÖ **Tiempo real** - Transcripci√≥n mientras hablas
- ‚úÖ **Espa√±ol** - Configurado para espa√±ol (es-ES)

**Navegadores compatibles:**
- ‚úÖ Chrome
- ‚úÖ Edge
- ‚úÖ Safari (con webkit)
- ‚ö†Ô∏è Firefox (soporte limitado)

No necesitas instalar nada adicional.

## ‚úÖ Estado Actual del Proyecto

- ‚úÖ Ollama configurado para evaluaci√≥n de recitaciones
- ‚úÖ Modelo por defecto: `llama3.1`
- ‚úÖ Transcripci√≥n con Web Speech API (integrada en navegador)
- ‚úÖ Todo funciona localmente sin APIs externas
- ‚úÖ Cero configuraci√≥n adicional necesaria

## üîÑ Pr√≥ximos Pasos Recomendados

1. **Instalar modelo recomendado:**

   ```bash
   ollama pull llama3.1
   ```

2. **Verificar que Ollama est√° corriendo:**

   ```bash
   ollama list
   ```

3. **Probar el sistema:**
   - Crear un chat de estudio
   - Agregar material
   - Click en "Empezar a hablar"
   - Explicar el material en voz alta
   - Ver el feedback de Ollama en tiempo real

4. **¬°Listo!** No necesitas configurar nada m√°s
