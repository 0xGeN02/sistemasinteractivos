#!/bin/bash

# Script de verificaci√≥n para el sistema de an√°lisis de presentaciones

echo "üîç Verificando sistema de an√°lisis de presentaciones..."
echo ""

# Verificar Node.js
echo "‚úì Node.js version:"
node --version
echo ""

# Verificar dependencias npm
echo "‚úì Verificando dependencias npm..."
if [ -d "node_modules" ]; then
    echo "  - node_modules existe"
    
    if [ -d "node_modules/multer" ]; then
        echo "  - multer instalado ‚úì"
    else
        echo "  - multer NO instalado ‚úó"
        echo "    Ejecuta: npm install"
    fi
    
    if [ -d "node_modules/ollama" ]; then
        echo "  - ollama instalado ‚úì"
    else
        echo "  - ollama NO instalado ‚úó"
        echo "    Ejecuta: npm install"
    fi
else
    echo "  - node_modules NO existe ‚úó"
    echo "    Ejecuta: npm install"
fi
echo ""

# Verificar directorio temp
echo "‚úì Verificando directorios..."
if [ -d "temp" ]; then
    echo "  - temp/ existe ‚úì"
else
    echo "  - temp/ NO existe, creando..."
    mkdir -p temp
    echo "  - temp/ creado ‚úì"
fi
echo ""

# Verificar Ollama
echo "‚úì Verificando Ollama..."
if command -v ollama &> /dev/null; then
    echo "  - Ollama CLI instalado ‚úì"
    
    # Verificar si Ollama est√° corriendo
    if curl -s http://localhost:11434/api/tags &> /dev/null; then
        echo "  - Ollama server corriendo ‚úì"
        
        # Verificar modelos
        echo ""
        echo "  Modelos disponibles:"
        curl -s http://localhost:11434/api/tags | grep -o '"name":"[^"]*"' | cut -d'"' -f4 | sed 's/^/    - /'
        
        echo ""
        echo "  üí° Modelos recomendados:"
        echo "    - Para texto: llama3.1, mistral, phi3"
        echo "    - Para video: llava, bakllava"
        echo ""
        echo "  Para instalar un modelo:"
        echo "    ollama pull llama3.1"
        echo "    ollama pull llava"
    else
        echo "  - Ollama server NO est√° corriendo ‚úó"
        echo "    Inicia Ollama con: ollama serve"
    fi
else
    echo "  - Ollama NO instalado ‚úó"
    echo "    Instala desde: https://ollama.ai"
fi
echo ""

# Verificar archivo .env
echo "‚úì Verificando configuraci√≥n..."
if [ -f ".env" ]; then
    echo "  - .env existe ‚úì"
    
    if grep -q "OLLAMA_MODEL" .env; then
        MODEL=$(grep "OLLAMA_MODEL" .env | cut -d'=' -f2)
        echo "  - OLLAMA_MODEL configurado: $MODEL"
    else
        echo "  - OLLAMA_MODEL no configurado (usar√° llama3.1 por defecto)"
    fi
    
    if grep -q "VISION_MODEL" .env; then
        VISION=$(grep "VISION_MODEL" .env | cut -d'=' -f2)
        echo "  - VISION_MODEL configurado: $VISION"
    else
        echo "  - VISION_MODEL no configurado (usar√° llava por defecto)"
    fi
else
    echo "  - .env NO existe"
    echo "  - Creando .env con valores por defecto..."
    echo "OLLAMA_MODEL=llama3.1" > .env
    echo "VISION_MODEL=llava" >> .env
    echo "  - .env creado ‚úì"
fi
echo ""

echo "=========================================="
echo "üìã Resumen"
echo "=========================================="
echo ""
echo "Para usar el an√°lisis de presentaciones:"
echo ""
echo "1. Aseg√∫rate de que Ollama est√© corriendo:"
echo "   ollama serve"
echo ""
echo "2. Instala los modelos necesarios:"
echo "   ollama pull llama3.1  # Para an√°lisis de texto"
echo "   ollama pull llava     # Para an√°lisis de video"
echo ""
echo "3. Inicia el servidor:"
echo "   npm run dev:full"
echo ""
echo "4. En la aplicaci√≥n:"
echo "   - Ve a la secci√≥n de Estudio"
echo "   - Carga tu material"
echo "   - (Opcional) Activa la c√°mara"
echo "   - Haz clic en 'Empezar a hablar'"
echo "   - Presenta tu temario"
echo "   - Haz clic en 'Detener y evaluar'"
echo ""
echo "=========================================="
